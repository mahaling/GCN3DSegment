{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X2Fj4S3r0p1A"
   },
   "source": [
    "##### Copyright 2019 Google LLC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "Okg-R95R1CaX"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t4v1coMcWtiJ"
   },
   "source": [
    "# Mesh Segmentation using Feature Steered Graph Convolutions\n",
    "\n",
    "Segmenting a mesh to its semantic parts is an important problem for 3D shape\n",
    "understanding. This colab demonstrates how to build a semantic mesh segmentation\n",
    "model for deformable shapes using graph convolution layers defined in\n",
    "[Tensorflow Graphics](https://github.com/tensorflow/graphics).\n",
    "\n",
    "![](https://storage.googleapis.com/tensorflow-graphics/notebooks/mesh_segmentation/mesh_segmentation_demo.png)\n",
    "\n",
    "This notebook covers the following key topics:\n",
    "* How to use graph-convolutional layers to define a CNN for mesh segmentation.\n",
    "* How to setup a data pipeline torepresent mesh connectivity with SparseTensors.\n",
    "\n",
    "Note: The easiest way to use this tutorial is as a Colab notebook, which allows\n",
    "you to dive in with no setup.\n",
    "\n",
    "### Image Convolutions vs Graph Convolutions\n",
    "\n",
    "Images are represented by uniform grids of pixels. Running convolutions on\n",
    "uniform grids is a well understood process and is at the core of a significant\n",
    "amount of products and academic publications.\n",
    "![](https://storage.googleapis.com/tensorflow-graphics/notebooks/mesh_segmentation/cat_image_convolutions.png)\n",
    "\n",
    "However, things become a bit more complicated when dealing with three\n",
    "dimensional objects such as meshes or point clouds since these are not defined\n",
    "on regular grids. A convolution operation for meshes or point clouds must\n",
    "operate on irregular data structures. This makes convolutional neural\n",
    "networks based on them harder to implement.\n",
    "![](https://storage.googleapis.com/tensorflow-graphics/notebooks/mesh_segmentation/cat_mesh_convolutions.png)\n",
    "\n",
    "Any general mesh can be denoted as a graph that is not constrained to a regular grid. Many graph-convolutional operators have been published in\n",
    "the recent years. In this demo we use the method described in\n",
    "[Feature Steered Graph Convolutions](https://arxiv.org/abs/1706.05206). Similar\n",
    "to it's image counterpart, this basic building block can be used do solve a\n",
    "plethora of problems. This Colab focusses on segmenting deformable meshes of\n",
    "human bodies into parts (e.g. head, right foot, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PNQ29y8Q4_cH"
   },
   "source": [
    "## Setup & Imports\n",
    "\n",
    "To run this Colab optimally, please update the runtime type to use a GPU\n",
    "hardware accelerator. - click on the 'Runtime' menu, then 'Change runtime type':\n",
    "\n",
    "![](https://storage.googleapis.com/tensorflow-graphics/notebooks/non_rigid_deformation/change_runtime.jpg)\n",
    "\n",
    "-   finally, set the 'Hardware accelerator' to 'GPU'.\n",
    "\n",
    "![](https://storage.googleapis.com/tensorflow-graphics/notebooks/mesh_segmentation/gpu_runtime.png)\n",
    "\n",
    "If Tensorflow Graphics is not installed on your system, the following cell will\n",
    "install the Tensorflow Graphics package for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UkPKOuyJKuKM"
   },
   "source": [
    "Now that Tensorflow Graphics and dependencies are installed, let's import everything needed to run the demos contained in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KlBviBxue7n0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Warning: To use the exr data format, please install the OpenEXR package following the instructions detailed in the README at github.com/tensorflow/graphics.\n",
      "Warning: To use the threejs_vizualization, please install the colabtools package following the instructions detailed in the README at github.com/tensorflow/graphics.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0912 21:41:55.204663 4534560192 scad.py:23] searching for scad in: /usr/local/bin:/Users/vrlab/anaconda3/condabin:/Users/vrlab/Library/Android/sdk/platform-tools:/usr/local/opt/gettext/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Applications/VMware Fusion.app/Contents/Public:/Library/TeX/texbin:/opt/X11/bin:/Library/Frameworks/Mono.framework/Versions/Current/Commands:/Applications/OpenSCAD.app/Contents/MacOS\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow_graphics.nn.layer import graph_convolution as graph_conv\n",
    "from tensorflow_graphics.notebooks import mesh_segmentation_dataio as dataio\n",
    "from tensorflow_graphics.notebooks import mesh_viewer\n",
    "%load_ext tensorboard\n",
    "\n",
    "import numpy as np\n",
    "import trimesh\n",
    "from mesh_visualization import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable autoload, since external python file will be changed after imported\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport mesh_visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AGaDtH49dlJb"
   },
   "source": [
    "Note this notebook works best in Graph mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Gh-ZSwXnB-5"
   },
   "source": [
    "### Fetch model files and data\n",
    "\n",
    "For convenience, we provide a pre-trained model. Let's now download a pre-trained model checkpoint and the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ZkB3iIcvvzJ"
   },
   "outputs": [],
   "source": [
    "# path_to_model_zip = tf.keras.utils.get_file(\n",
    "#     'model.zip',\n",
    "#     origin='https://storage.googleapis.com/tensorflow-graphics/notebooks/mesh_segmentation/model.zip',\n",
    "#     extract=True)\n",
    "\n",
    "# path_to_data_zip = tf.keras.utils.get_file(\n",
    "#     'data.zip',\n",
    "#     origin='https://storage.googleapis.com/tensorflow-graphics/notebooks/mesh_segmentation/data.zip',\n",
    "#     extract=True)\n",
    "\n",
    "# local_model_dir = os.path.join(os.path.dirname(path_to_model_zip), 'model')\n",
    "# test_data_files = [\n",
    "#     os.path.join(\n",
    "#         os.path.dirname(path_to_data_zip),\n",
    "#         'data/Dancer_test_sequence.tfrecords')\n",
    "# ]\n",
    "\n",
    "test_data_files = ['Dancer_test_sequence.tfrecords']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dmh4b6VKcATt"
   },
   "source": [
    "## Load and visualize test data\n",
    "\n",
    "For graph convolutions, we need a *weighted adjacency matrix* denoting the mesh\n",
    "connectivity. Feature-steered graph convolutions expect self-edges in the mesh\n",
    "connectivity for each vertex, i.e. the diagonal of the weighted adjacency matrix\n",
    "should be non-zero. This matrix is defined as:\n",
    "```\n",
    "A[i, j] = w[i,j] if vertex i and vertex j share an edge,\n",
    "A[i, i] = w[i,i] for each vertex i,\n",
    "A[i, j] = 0 otherwise.\n",
    "where, w[i, j] = 1/(degree(vertex i)), and sum(j)(w[i,j]) = 1\n",
    "```\n",
    "Here degree(vertex i) is the number of edges incident on a vertex (including the\n",
    "self-edge). This weighted adjacency matrix is stored as a SparseTensor.\n",
    "\n",
    "We will load the test meshes from the test [tf.data.TFRecordDataset](https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset)\n",
    "downloaded above. Each mesh is stored as a\n",
    "[tf.Example](https://www.tensorflow.org/api_docs/python/tf/train/Example), with\n",
    "the following fields:\n",
    "\n",
    "*   'num_vertices': Number of vertices in each mesh\n",
    "*   'num_triangles': Number of triangles in each mesh.\n",
    "*   'vertices': A [V, 3] float tensor of vertex positions.\n",
    "*   'triangles': A [T, 3] integer tensor of vertex indices for each triangle.\n",
    "*   'labels': A [V] integer tensor with segmentation class label for each\n",
    "    vertex.\n",
    "\n",
    "where 'V' is number of vertices and 'T' is number of triangles in the mesh. As\n",
    "each mesh may have a varying number of vertices and faces (and the corresponding\n",
    "connectivity matrix), we pad the data tensors with '0's in each batch.\n",
    "\n",
    "For details on the dataset pipeline implementation, take a look at\n",
    "mesh_segmentation_dataio.py.\n",
    "\n",
    "Lets try to load a batch from the test TFRecordDataset, and visualize the first\n",
    "mesh with each vertex colored by the part label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LZM02o0pEny6"
   },
   "outputs": [],
   "source": [
    "test_io_params = {\n",
    "    'is_training': False,\n",
    "    'sloppy': False,\n",
    "    'shuffle': True,\n",
    "}\n",
    "test_tfrecords = test_data_files\n",
    "\n",
    "input_graph = tf.Graph()\n",
    "with input_graph.as_default():\n",
    "  mesh_load_op = create_input_from_dataset(\n",
    "      create_dataset_from_tfrecords, test_tfrecords, test_io_params)\n",
    "  with tf.Session() as sess:\n",
    "    test_mesh_data, test_labels = sess.run(mesh_load_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jgRp-0fplMBD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Current Index: 0\n",
      ">>>> Vertices: 2652 / 2663\n",
      ">>>> Faces: 3999 / 4000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04aa079010b749d8b41655a6234a0c24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(background='#dddddd', camera=PerspectiveCamera(aspect=1.5, children=(DirectionalLight(position=(-30.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   0. 223. 291. 484. 471. 819. 272.  84.   4.   2.]\n"
     ]
    }
   ],
   "source": [
    "if 'cur_idx' not in locals() and 'cur_idx' not in globals():\n",
    "  cur_idx = 0\n",
    "    \n",
    "if cur_idx >= test_mesh_data['num_vertices'].shape[0]:\n",
    "  cur_idx = 0\n",
    "\n",
    "print(\">>> Current Index: %d\" % cur_idx)\n",
    "print(\">>>> Vertices: %d / %d\" % (test_mesh_data['num_vertices'][cur_idx], test_mesh_data[\"vertices\"][cur_idx].shape[0]))\n",
    "print(\">>>> Faces: %d / %d\" % (test_mesh_data[\"num_triangles\"][cur_idx], test_mesh_data[\"triangles\"][cur_idx].shape[0]))\n",
    "\n",
    "input_mesh_data = {\n",
    "    'vertices': test_mesh_data['vertices'][cur_idx],\n",
    "    'faces': test_mesh_data['triangles'][cur_idx],\n",
    "    'vertex_colors': mesh_viewer.SEGMENTATION_COLORMAP[test_labels[cur_idx]],\n",
    "}\n",
    "display_mesh(input_mesh_data)\n",
    "\n",
    "mesh = trimesh.Trimesh(vertices=test_mesh_data['vertices'][cur_idx][:test_mesh_data['num_vertices'][cur_idx]],\n",
    "                       faces=test_mesh_data['triangles'][cur_idx][:test_mesh_data['num_triangles'][cur_idx]])\n",
    "neighbor_num = [len(v) for v in mesh.vertex_neighbors]\n",
    "hist, bins, _ = plt.hist(neighbor_num, bins=range(12), alpha=0.7, rwidth=0.75)  # arguments are passed to np.histogram\n",
    "plt.title(\"Histogram of count of neighbors\")\n",
    "plt.show()\n",
    "\n",
    "# hist, bins = np.histogram(neighbor_num, bins=range(12))\n",
    "print(hist)\n",
    "\n",
    "cur_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aqV6vkCkWB7J"
   },
   "source": [
    "## Model Definition\n",
    "\n",
    "Given a mesh with V vertices and D-dimensional per-vertex input features (e.g.\n",
    "vertex position, normal), we would like to create a network capable of\n",
    "classifying each vertex to a part label. Lets first create a mesh encoder that\n",
    "encodes each vertex in the mesh into C-dimensional logits, where C is the number\n",
    "of parts. First we use 1x1 convolutions to change input feature dimensions,\n",
    "followed by a sequence of feature steered graph convolutions and ReLU\n",
    "non-linearities, and finally 1x1 convolutions to logits, which are used for\n",
    "computing softmax cross entropy as described below.\n",
    "\n",
    "Note that this model does not use any form of pooling, which is outside the scope of this notebook.\n",
    "\n",
    "![](https://storage.googleapis.com/tensorflow-graphics/notebooks/mesh_segmentation/mesh_segmentation_model_def.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fQVeuGazM0LK"
   },
   "outputs": [],
   "source": [
    "MODEL_PARAMS = {\n",
    "    'num_filters': 8,\n",
    "    'num_classes': 4,\n",
    "    'encoder_filter_dims': [32, 64, 128, 256, 1024],\n",
    "    \n",
    "    # Adam Optimizer\n",
    "    'learning_rate': 0.001,\n",
    "    'beta': 0.9,\n",
    "    'adam_epsilon': 1e-08\n",
    "}\n",
    "\n",
    "\n",
    "def mesh_encoder(batch_mesh_data, num_filters, output_dim, conv_layer_dims):\n",
    "  \"\"\"A mesh encoder using feature steered graph convolutions.\n",
    "\n",
    "    The shorthands used below are\n",
    "      `B`: Batch size.\n",
    "      `V`: The maximum number of vertices over all meshes in the batch.\n",
    "      `D`: The number of dimensions of input vertex features, D=3 if vertex\n",
    "        positions are used as features.\n",
    "\n",
    "  Args:\n",
    "    batch_mesh_data: A mesh_data dict with following keys\n",
    "      'vertices': A [B, V, D] `float32` tensor of vertex features, possibly\n",
    "        0-padded.\n",
    "      'neighbors': A [B, V, V] `float32` sparse tensor of edge weights.\n",
    "      'num_vertices': A [B] `int32` tensor of number of vertices per mesh.\n",
    "    num_filters: The number of weight matrices to be used in feature steered\n",
    "      graph conv.\n",
    "    output_dim: A dimension of output per vertex features.\n",
    "    conv_layer_dims: A list of dimensions used in graph convolution layers.\n",
    "\n",
    "  Returns:\n",
    "    vertex_features: A [B, V, output_dim] `float32` tensor of per vertex\n",
    "      features.\n",
    "  \"\"\"\n",
    "  batch_vertices = batch_mesh_data['vertices']\n",
    "\n",
    "  # Linear: N x D --> N x 16.\n",
    "  vertex_features = tf.keras.layers.Conv1D(16, 1, name='lin16')(batch_vertices)\n",
    "\n",
    "  # graph convolution layers\n",
    "  for dim in conv_layer_dims:\n",
    "    with tf.variable_scope('conv_%d' % dim):\n",
    "      vertex_features = graph_conv.feature_steered_convolution_layer(\n",
    "          vertex_features,\n",
    "          batch_mesh_data['neighbors'],\n",
    "          batch_mesh_data['num_vertices'],\n",
    "          num_weight_matrices=num_filters,\n",
    "          num_output_channels=dim)\n",
    "    vertex_features = tf.nn.relu(vertex_features)\n",
    "\n",
    "  # Linear: N x 128 --> N x 256.\n",
    "  vertex_features = tf.keras.layers.Conv1D(\n",
    "      256, 1, name='lin256')(\n",
    "          vertex_features)\n",
    "  vertex_features = tf.nn.relu(vertex_features)\n",
    "\n",
    "  # Linear: N x 256 --> N x output_dim.\n",
    "  vertex_features = tf.keras.layers.Conv1D(\n",
    "      output_dim, 1, name='lin_output')(\n",
    "          vertex_features)\n",
    "\n",
    "  return vertex_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6c2pz4r_79F_"
   },
   "source": [
    "Given a mesh encoder, let's define a model_fn for a custom\n",
    "[tf.Estimator](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator)\n",
    "for vertex classification using softmax cross entropy loss. A tf.Estimator model_fn returns the ops necessary to perform training, evaluation, or predictions given inputs and a number of other parameters. Recall that the\n",
    "vertex tensor may be zero-padded (see Dataset Pipeline above), hence we must mask out the contribution from the padded values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WE-cuv0i78ak"
   },
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "  \"\"\"Returns a mesh segmentation model_fn for use with tf.Estimator.\"\"\"\n",
    "  logits = mesh_encoder(features, params['num_filters'], params['num_classes'],\n",
    "                        params['encoder_filter_dims'])\n",
    "  predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "  outputs = {\n",
    "      'vertices': features['vertices'],\n",
    "      'triangles': features['triangles'],\n",
    "      'num_vertices': features['num_vertices'],\n",
    "      'num_triangles': features['num_triangles'],\n",
    "      'predictions': predictions\n",
    "  }\n",
    "  # For predictions, return the outputs.\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    outputs['labels'] = features['labels']\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=outputs)\n",
    "  # Loss\n",
    "  # Weight the losses by masking out padded vertices/labels.\n",
    "  vertex_ragged_sizes = features['num_vertices']\n",
    "  mask = tf.sequence_mask(vertex_ragged_sizes, tf.shape(labels)[-1])\n",
    "  loss_weights = tf.cast(mask, dtype=tf.float32)\n",
    "  loss = tf.losses.sparse_softmax_cross_entropy(\n",
    "      logits=logits, labels=labels, weights=loss_weights)\n",
    "  # For training, build the optimizer.\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.AdamOptimizer(\n",
    "        learning_rate=params['learning_rate'],\n",
    "        beta1=params['beta'],\n",
    "        epsilon=params['adam_epsilon'])\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "      train_op = optimizer.minimize(\n",
    "          loss=loss, global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "  # For eval, return eval metrics.\n",
    "  eval_ops = {\n",
    "      'mean_loss':\n",
    "          tf.metrics.mean(loss),\n",
    "      'accuracy':\n",
    "          tf.metrics.accuracy(\n",
    "              labels=labels, predictions=predictions, weights=loss_weights)\n",
    "  }\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "94FICCro_dLV"
   },
   "source": [
    "## Test model & visualize results\n",
    "\n",
    "Now that we have defined the model, let's load the weights from the trained model downloaded above and use tf.Estimator.predict to predict the part labels for meshes in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Olj5zIkg72FK"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a44d36d7636c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m }\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtest_tfrecords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data_files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tfrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data_files' is not defined"
     ]
    }
   ],
   "source": [
    "test_io_params = {\n",
    "    'is_training': False,\n",
    "    'sloppy': False,\n",
    "    'shuffle': True,\n",
    "    'repeat': False,\n",
    "    'batch_size': 5\n",
    "}\n",
    "test_tfrecords = test_data_files\n",
    "print(test_tfrecords)\n",
    "\n",
    "def predict_fn():\n",
    "  return create_input_from_dataset(create_dataset_from_tfrecords,\n",
    "                                          test_tfrecords,\n",
    "                                          test_io_params)\n",
    "\n",
    "\n",
    "estimator = tf.estimator.Estimator(model_fn=model_fn,\n",
    "                                   model_dir=local_model_dir,\n",
    "                                   params=MODEL_PARAMS)\n",
    "test_predictions = estimator.predict(input_fn=predict_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IO1VmbL087xf"
   },
   "source": [
    "Run the following cell repeatedly to cycle through the meshes in the test sequence. The left view shows the input mesh, and the right view shows the predicted part labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xuoVe70D5PAF"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7692ac66f9d4b25aae6ec17ec60c841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(background='#dddddd', camera=PerspectiveCamera(aspect=1.5, children=(DirectionalLight(position=(-30.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9581de21a6094d1b957f6a9608bfa3a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(background='#dddddd', camera=PerspectiveCamera(aspect=1.5, children=(DirectionalLight(position=(-30.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "prediction = next(test_predictions)\n",
    "# print(prediction[\"extra\"])\n",
    "input_mesh_data = {\n",
    "    'vertices': prediction['vertices'],\n",
    "    'faces': prediction['triangles'],\n",
    "}\n",
    "predicted_mesh_data = {\n",
    "    'vertices': prediction['vertices'],\n",
    "    'faces': prediction['triangles'],\n",
    "    'vertex_colors': mesh_viewer.SEGMENTATION_COLORMAP[prediction['predictions']],\n",
    "}\n",
    "\n",
    "display_mesh(input_mesh_data)\n",
    "display_mesh(predicted_mesh_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on domeCloth Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_io_params = {\n",
    "    'is_training': False,\n",
    "    'sloppy': False,\n",
    "    'shuffle': True,\n",
    "}\n",
    "test_tfrecords = ['psbCup.train.tfrecords']\n",
    "\n",
    "input_graph = tf.Graph()\n",
    "with input_graph.as_default():\n",
    "  mesh_load_op = dataio.create_input_from_dataset(\n",
    "      dataio.create_dataset_from_tfrecords, test_tfrecords, test_io_params)\n",
    "  with tf.Session() as sess:\n",
    "    test_mesh_data, test_labels = sess.run(mesh_load_op)\n",
    "    \n",
    "cur_model_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/threevis/mesh_helper.py:47: RuntimeWarning: invalid value encountered in true_divide\n",
      "  norm = norm / np.linalg.norm(norm)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "156f8cc414fb4085ad84c2e3bfc565f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(background='#dddddd', camera=PerspectiveCamera(aspect=1.5, children=(DirectionalLight(position=(-30.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_mesh_data = {\n",
    "    'vertices': test_mesh_data['vertices'][cur_model_idx],\n",
    "    'faces': test_mesh_data['triangles'][cur_model_idx],\n",
    "    'vertex_colors': mesh_viewer.SEGMENTATION_COLORMAP[test_labels[cur_model_idx]],\n",
    "}\n",
    "display_mesh(input_mesh_data)\n",
    "\n",
    "cur_model_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Cup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0827 10:47:28.753488 4764538304 estimator.py:1790] Using default config.\n",
      "I0827 10:47:28.756253 4764538304 estimator.py:209] Using config: {'_model_dir': './smallDome_model_1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x141efd6d8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "W0827 10:47:28.763701 4764538304 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "I0827 10:47:28.934149 4764538304 estimator.py:1145] Calling model_fn.\n",
      "W0827 10:47:28.935016 4764538304 deprecation.py:506] From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "I0827 10:47:31.934803 4764538304 estimator.py:1147] Done calling model_fn.\n",
      "I0827 10:47:31.936692 4764538304 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
      "I0827 10:47:32.663175 4764538304 monitored_session.py:240] Graph was finalized.\n",
      "I0827 10:47:33.043652 4764538304 session_manager.py:500] Running local_init_op.\n",
      "I0827 10:47:33.113375 4764538304 session_manager.py:502] Done running local_init_op.\n",
      "I0827 10:47:35.191325 4764538304 basic_session_run_hooks.py:606] Saving checkpoints for 0 into ./smallDome_model_1/model.ckpt.\n"
     ]
    }
   ],
   "source": [
    "train_io_params = {\n",
    "    'is_training': True,\n",
    "    'sloppy': False,\n",
    "    'shuffle': True,\n",
    "    'repeat': True,\n",
    "    'batch_size': 5\n",
    "}\n",
    "train_tfrecords = ['domecloth.tfrecords']\n",
    "\n",
    "# Show training progress\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "def train_fn():\n",
    "  return dataio.create_input_from_dataset(dataio.create_dataset_from_tfrecords,\n",
    "                                          train_tfrecords,\n",
    "                                          train_io_params)\n",
    "local_model_dir = \"./smallDome_model_1\"\n",
    "\n",
    "estimator = tf.estimator.Estimator(model_fn=model_fn,\n",
    "                                   model_dir=local_model_dir,\n",
    "                                   params=MODEL_PARAMS)\n",
    "estimator.train(input_fn=train_fn, steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Cup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0819 16:51:16.819298 4673811904 estimator.py:1790] Using default config.\n",
      "I0819 16:51:16.820495 4673811904 estimator.py:209] Using config: {'_model_dir': './cosegCup_model_1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1455e17b8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "test_io_params = {\n",
    "    'is_training': False,\n",
    "    'sloppy': False,\n",
    "    'shuffle': True,\n",
    "}\n",
    "test_tfrecords = ['cosegCup.test.tfrecords']\n",
    "\n",
    "def predict_fn():\n",
    "  return dataio.create_input_from_dataset(dataio.create_dataset_from_tfrecords,\n",
    "                                          test_tfrecords,\n",
    "                                          test_io_params)\n",
    "local_model_dir = \"./cosegCup_model_1\"\n",
    "\n",
    "estimator = tf.estimator.Estimator(model_fn=model_fn,\n",
    "                                   model_dir=local_model_dir,\n",
    "                                   params=MODEL_PARAMS)\n",
    "test_predictions = estimator.predict(input_fn=predict_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b81ed75b284fbe8703b8827ef9e704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(background='#dddddd', camera=PerspectiveCamera(aspect=1.5, children=(DirectionalLight(position=(-30.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c481526e664007abc9719c1cd4a101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(background='#dddddd', camera=PerspectiveCamera(aspect=1.5, children=(DirectionalLight(position=(-30.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction = next(test_predictions)\n",
    "# print(prediction[\"extra\"])\n",
    "input_mesh_data = {\n",
    "    'vertices': prediction['vertices'],\n",
    "    'faces': prediction['triangles'],\n",
    "    'vertex_colors': mesh_viewer.SEGMENTATION_COLORMAP[prediction['labels']],\n",
    "}\n",
    "predicted_mesh_data = {\n",
    "    'vertices': prediction['vertices'],\n",
    "    'faces': prediction['triangles'],\n",
    "    'vertex_colors': mesh_viewer.SEGMENTATION_COLORMAP[prediction['predictions']],\n",
    "}\n",
    "\n",
    "display_mesh(input_mesh_data)\n",
    "display_mesh(predicted_mesh_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with Our Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4vYRrD4-POtR"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a15c4774ab45418448e3cf0b53f92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(background='#dddddd', camera=PerspectiveCamera(aspect=1.5, children=(DirectionalLight(position=(-30.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mesh = trimesh.load(\"../meshes/f_c_10412256613_model.obj\")\n",
    "mesh_data = {\n",
    "    'vertices': mesh.vertices,\n",
    "    'faces': mesh.faces\n",
    "}\n",
    "display_mesh(mesh_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3xYrAxxPa5iE"
   },
   "outputs": [],
   "source": [
    "mean_center = True\n",
    "\n",
    "def mesh_to_features(mesh):\n",
    "  labels = tf.zeros(mesh.vertices.shape[0], tf.int32)\n",
    "  vertices = tf.convert_to_tensor(mesh.vertices, dtype=tf.float32)\n",
    "  num_vertices = tf.shape(input=vertices)[0]\n",
    "  if mean_center:\n",
    "    vertices = vertices - tf.reduce_mean(input_tensor=vertices, axis=0, keepdims=True)\n",
    "\n",
    "  triangles = tf.convert_to_tensor(mesh.faces, dtype=tf.int32)\n",
    "  num_triangles = tf.shape(input=triangles)[0]\n",
    "\n",
    "  edges, edge_weights = tf.py_function(\n",
    "    func=lambda t: dataio.get_weighted_edges(t.numpy()),\n",
    "    inp=[triangles],\n",
    "    Tout=[tf.int32, tf.float32]\n",
    "  )\n",
    "  num_edges = tf.shape(input=edges)[0]\n",
    "\n",
    "  vertices=tf.reshape(vertices, [1, -1, 3])\n",
    "  triangles=tf.reshape(triangles, [1, -1, 3])\n",
    "  edges=tf.reshape(edges, [1, -1, 2])\n",
    "  edge_weights=tf.reshape(edge_weights, [1, -1])\n",
    "  labels = tf.reshape(labels, [1, -1])\n",
    "  \n",
    "  num_vertices = tf.reshape(num_vertices, [1])\n",
    "  num_triangles = tf.reshape(num_triangles, [1])\n",
    "  num_edges = tf.reshape(num_edges, [1])\n",
    "\n",
    "#     paddings = tf.constant([[0, 2667 - vertices.shape[0]], [0, 0]])\n",
    "#     vertices = tf.pad(vertices, paddings, \"CONSTANT\")\n",
    "\n",
    "#     paddings = tf.constant([[0, 4000 - triangles.shape[0]], [0, 0]])\n",
    "#     triangles = tf.pad(vertices, paddings, \"CONSTANT\")\n",
    "\n",
    "#     paddings = tf.constant([[0, 15877 - edges.shape[0]], [0, 0]])\n",
    "#     edges = tf.pad(edges, paddings, \"CONSTANT\")\n",
    "\n",
    "#     paddings = tf.constant([[0, 15877 - weights.shape[0]], [0, 0]])\n",
    "#     weights = tf.pad(weights, paddings, \"CONSTANT\")\n",
    "\n",
    "  neighbors = dataio.adjacency_from_edges(\n",
    "    edges,\n",
    "    edge_weights,\n",
    "    num_edges,\n",
    "    num_vertices\n",
    "  )\n",
    "\n",
    "\n",
    "  features = dict(\n",
    "    vertices=vertices,\n",
    "    triangles=triangles,\n",
    "    neighbors=neighbors,\n",
    "    num_triangles=num_triangles,\n",
    "    num_vertices=num_vertices,\n",
    "    labels=labels\n",
    "  )\n",
    "\n",
    "\n",
    "#   return labels, vertices, triangles, edges, edge_weights, neighbors\n",
    "  return features, labels\n",
    "\n",
    "# input_graph = tf.Graph()\n",
    "# with input_graph.as_default():\n",
    "#   with tf.Session() as sess:\n",
    "#     mesh_op = handle(mesh)\n",
    "#     labels, vertices, triangles, edges, weights, neighbors = sess.run(mesh_op)\n",
    "\n",
    "#     print(vertices.shape)\n",
    "#     print(edges.shape)\n",
    "#     print(weights.shape)\n",
    "#     print(neighbors)\n",
    "    \n",
    "# mesh_data = {\n",
    "#     'vertices': vertices,\n",
    "#     'faces': triangles\n",
    "# }\n",
    "# input_viewer = mesh_viewer.Viewer(mesh_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4lWNHEaNuIyp"
   },
   "outputs": [],
   "source": [
    "def my_predict_fn():\n",
    "  return mesh_to_features(mesh)\n",
    "\n",
    "\n",
    "my_estimator = tf.estimator.Estimator(model_fn=model_fn,\n",
    "                                   model_dir=local_model_dir,\n",
    "                                   params=MODEL_PARAMS)\n",
    "my_test_predictions = my_estimator.predict(input_fn=my_predict_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6VUHIBHuudHG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0808 17:41:16.844424 4485752256 estimator.py:1000] Input graph does not use tf.data.Dataset or contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Vertices: 67149 / 67149\n",
      ">>>> Faces: 100000 / 100000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a06fef75c1b442dad274cbf07ad33a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(background='#dddddd', camera=PerspectiveCamera(aspect=1.5, children=(DirectionalLight(position=(-30.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe53f0220724f05a553e9c8cc8e2a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(background='#dddddd', camera=PerspectiveCamera(aspect=1.5, children=(DirectionalLight(position=(-30.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction = next(my_test_predictions)\n",
    "print(\">>>> Vertices: %d / %d\" % (prediction[\"num_vertices\"], prediction[\"vertices\"].shape[0]))\n",
    "print(\">>>> Faces: %d / %d\" % (prediction[\"num_triangles\"], prediction[\"triangles\"].shape[0]))\n",
    "\n",
    "input_mesh_data = {\n",
    "    'vertices': prediction['vertices'],\n",
    "    'faces': prediction['triangles'],\n",
    "}\n",
    "predicted_mesh_data = {\n",
    "    'vertices': prediction['vertices'],\n",
    "    'faces': prediction['triangles'],\n",
    "    'vertex_colors': mesh_viewer.SEGMENTATION_COLORMAP[prediction['predictions']],\n",
    "}\n",
    "\n",
    "display_mesh(input_mesh_data)\n",
    "display_mesh(predicted_mesh_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of mesh_segmentation_demo.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
