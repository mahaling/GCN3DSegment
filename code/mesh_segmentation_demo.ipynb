{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X2Fj4S3r0p1A"
   },
   "source": [
    "##### Copyright 2019 Google LLC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "Okg-R95R1CaX"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t4v1coMcWtiJ"
   },
   "source": [
    "# Mesh Segmentation using Feature Steered Graph Convolutions\n",
    "\n",
    "Segmenting a mesh to its semantic parts is an important problem for 3D shape\n",
    "understanding. This colab demonstrates how to build a semantic mesh segmentation\n",
    "model for deformable shapes using graph convolution layers defined in\n",
    "[Tensorflow Graphics](https://github.com/tensorflow/graphics).\n",
    "\n",
    "![](https://storage.googleapis.com/tensorflow-graphics/notebooks/mesh_segmentation/mesh_segmentation_demo.png)\n",
    "\n",
    "This notebook covers the following key topics:\n",
    "* How to use graph-convolutional layers to define a CNN for mesh segmentation.\n",
    "* How to setup a data pipeline torepresent mesh connectivity with SparseTensors.\n",
    "\n",
    "Note: The easiest way to use this tutorial is as a Colab notebook, which allows\n",
    "you to dive in with no setup.\n",
    "\n",
    "### Image Convolutions vs Graph Convolutions\n",
    "\n",
    "Images are represented by uniform grids of pixels. Running convolutions on\n",
    "uniform grids is a well understood process and is at the core of a significant\n",
    "amount of products and academic publications.\n",
    "![](https://storage.googleapis.com/tensorflow-graphics/notebooks/mesh_segmentation/cat_image_convolutions.png)\n",
    "\n",
    "However, things become a bit more complicated when dealing with three\n",
    "dimensional objects such as meshes or point clouds since these are not defined\n",
    "on regular grids. A convolution operation for meshes or point clouds must\n",
    "operate on irregular data structures. This makes convolutional neural\n",
    "networks based on them harder to implement.\n",
    "![](https://storage.googleapis.com/tensorflow-graphics/notebooks/mesh_segmentation/cat_mesh_convolutions.png)\n",
    "\n",
    "Any general mesh can be denoted as a graph that is not constrained to a regular grid. Many graph-convolutional operators have been published in\n",
    "the recent years. In this demo we use the method described in\n",
    "[Feature Steered Graph Convolutions](https://arxiv.org/abs/1706.05206). Similar\n",
    "to it's image counterpart, this basic building block can be used do solve a\n",
    "plethora of problems. This Colab focusses on segmenting deformable meshes of\n",
    "human bodies into parts (e.g. head, right foot, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PNQ29y8Q4_cH"
   },
   "source": [
    "## Setup & Imports\n",
    "\n",
    "To run this Colab optimally, please update the runtime type to use a GPU\n",
    "hardware accelerator. - click on the 'Runtime' menu, then 'Change runtime type':\n",
    "\n",
    "![](https://storage.googleapis.com/tensorflow-graphics/notebooks/non_rigid_deformation/change_runtime.jpg)\n",
    "\n",
    "-   finally, set the 'Hardware accelerator' to 'GPU'.\n",
    "\n",
    "![](https://storage.googleapis.com/tensorflow-graphics/notebooks/mesh_segmentation/gpu_runtime.png)\n",
    "\n",
    "If Tensorflow Graphics is not installed on your system, the following cell will\n",
    "install the Tensorflow Graphics package for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UkPKOuyJKuKM"
   },
   "source": [
    "Now that Tensorflow Graphics and dependencies are installed, let's import everything needed to run the demos contained in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KlBviBxue7n0"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow_graphics.nn.layer import graph_convolution as graph_conv\n",
    "from tensorflow_graphics.notebooks import mesh_segmentation_dataio as dataio\n",
    "from tensorflow_graphics.notebooks import mesh_viewer\n",
    "\n",
    "import numpy as np\n",
    "import trimesh\n",
    "from mesh_viewer import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow_graphics.geometry.convolution import utils as conv_utils\n",
    "from tensorflow_graphics.geometry.representation.mesh import utils as mesh_utils\n",
    "from tensorflow_graphics.util import shape\n",
    "\n",
    "DEFAULT_IO_PARAMS = {\n",
    "    'batch_size': 8,\n",
    "    'shuffle_buffer_size': 100,\n",
    "    'is_training': True,\n",
    "    'parallel_threads': 5,\n",
    "    'mean_center': True,\n",
    "    'shuffle': None,\n",
    "    'repeat': None,\n",
    "}\n",
    "\n",
    "\n",
    "def adjacency_from_edges(edges, weights, num_edges, num_vertices):\n",
    "  \"\"\"Returns a batched sparse 1-ring adj tensor from edge list tensor.\n",
    "  Args:\n",
    "    edges: [B, E, 2] `int32` tensor of edges, possibly 0 padded.\n",
    "    weights: [B, E] `float32` tensor of edge weights, possibly 0 padded.\n",
    "    num_edges: [B] `int32` tensor of number of valid edges per batch sample.\n",
    "    num_vertices: [B] `int32` tensor of number of valid vertices per batch\n",
    "      sample.\n",
    "  Returns:\n",
    "    adj: A batched SparseTensor of weighted adjacency graph, of\n",
    "      dense_shape [B, V, V] where V is max(num_vertices)\n",
    "  \"\"\"\n",
    "  edges = tf.convert_to_tensor(value=edges)\n",
    "  weights = tf.convert_to_tensor(value=weights)\n",
    "  num_edges = tf.convert_to_tensor(value=num_edges)\n",
    "  num_vertices = tf.convert_to_tensor(value=num_vertices)\n",
    "\n",
    "  if not edges.dtype.is_integer:\n",
    "    raise TypeError(\"'edges' must have an integer type.\")\n",
    "  if not num_edges.dtype.is_integer:\n",
    "    raise TypeError(\"'num_edges' must have an integer type.\")\n",
    "  if not num_vertices.dtype.is_integer:\n",
    "    raise TypeError(\"'num_vertices' must have an integer type.\")\n",
    "  if not weights.dtype.is_floating:\n",
    "    raise TypeError(\"'weights' must have a floating type.\")\n",
    "\n",
    "  shape.check_static(tensor=edges, tensor_name='edges', has_rank=3)\n",
    "  shape.check_static(tensor=weights, tensor_name='weights', has_rank=2)\n",
    "  shape.check_static(tensor=num_edges, tensor_name='num_edges', has_rank=1)\n",
    "  shape.check_static(\n",
    "      tensor=num_vertices, tensor_name='num_vertices', has_rank=1)\n",
    "  shape.compare_dimensions(\n",
    "      tensors=(edges, weights, num_edges, num_vertices),\n",
    "      tensor_names=('edges', 'weights', 'num_edges', 'num_vertices'),\n",
    "      axes=(-3, -2, -1, -1))\n",
    "  shape.compare_dimensions(\n",
    "      tensors=(edges, weights),\n",
    "      tensor_names=('edges', 'weights'),\n",
    "      axes=(-2, -1))\n",
    "\n",
    "  batch_size = tf.shape(input=edges)[0]\n",
    "  max_num_vertices = tf.reduce_max(input_tensor=num_vertices)\n",
    "  max_num_edges = tf.shape(input=edges)[1]\n",
    "  batch_col = tf.reshape(tf.range(batch_size, dtype=edges.dtype), [-1, 1, 1])\n",
    "  batch_col = tf.tile(batch_col, [1, max_num_edges, 1])\n",
    "  batch_edges = tf.concat([batch_col, edges], axis=-1)\n",
    "\n",
    "  indices, _ = conv_utils.flatten_batch_to_2d(batch_edges, sizes=num_edges)\n",
    "  values, _ = conv_utils.flatten_batch_to_2d(\n",
    "      tf.expand_dims(weights, -1), sizes=num_edges)\n",
    "  values = tf.squeeze(values)\n",
    "  adjacency = tf.SparseTensor(\n",
    "      indices=tf.cast(indices, tf.int64),\n",
    "      values=values,\n",
    "      dense_shape=[batch_size, max_num_vertices, max_num_vertices])\n",
    "  adjacency = tf.sparse.reorder(adjacency)\n",
    "  return adjacency\n",
    "\n",
    "\n",
    "def get_weighted_edges(faces, self_edges=True):\n",
    "  r\"\"\"Gets unique edges and degree weights from a triangular mesh.\n",
    "  The shorthands used below are:\n",
    "      `T`: The number of triangles in the mesh.\n",
    "      `E`: The number of unique directed edges in the mesh.\n",
    "  Args:\n",
    "    faces: A [T, 3] `int32` numpy.ndarray of triangle vertex indices.\n",
    "    self_edges: A `bool` flag. If true, then for every vertex 'i' an edge\n",
    "      [i, i] is added to edge list.\n",
    "  Returns:\n",
    "    edges: A  [E, 2] `int32` numpy.ndarray of directed edges.\n",
    "    weights: A [E] `float32` numpy.ndarray denoting edge weights.\n",
    "    The degree of a vertex is the number of edges incident on the vertex,\n",
    "    including any self-edges. The weight for an edge $w_{ij}$ connecting vertex\n",
    "    $v_i$ and vertex $v_j$ is defined as,\n",
    "    $$\n",
    "    w_{ij} = 1.0 / degree(v_i)\n",
    "    \\sum_{j} w_{ij} = 1\n",
    "    $$\n",
    "  \"\"\"\n",
    "  edges = mesh_utils.extract_unique_edges_from_triangular_mesh(\n",
    "      faces, directed_edges=True).astype(np.int32)\n",
    "  if self_edges:\n",
    "    vertices = np.expand_dims(np.unique(edges[:, 0]), axis=1)\n",
    "    self_edges = np.concatenate((vertices, vertices), axis=1)\n",
    "    edges = np.unique(np.concatenate((edges, self_edges), axis=0), axis=0)\n",
    "  weights = mesh_utils.get_degree_based_edge_weights(edges, dtype=np.float32)\n",
    "  return edges, weights\n",
    "\n",
    "\n",
    "def _tfrecords_to_dataset(tfrecords,\n",
    "                          parallel_threads,\n",
    "                          shuffle,\n",
    "                          repeat,\n",
    "                          sloppy,\n",
    "                          max_readers=16):\n",
    "  \"\"\"Creates a TFRecordsDataset that iterates over filenames in parallel.\n",
    "  Args:\n",
    "    tfrecords: A list of tf.Data.TFRecords filenames.\n",
    "    parallel_threads: The `int` number denoting number of parallel worker\n",
    "      threads.\n",
    "    shuffle: The `bool` flag denoting whether to shuffle the dataset.\n",
    "    repeat: The `bool` flag denoting whether to repeat the dataset.\n",
    "    sloppy: The `bool` flag denoting if elements are produced in deterministic\n",
    "      order.\n",
    "    max_readers: The `int` number denoting the maximum number of input tfrecords\n",
    "      to interleave from in parallel.\n",
    "  Returns:\n",
    "    A tf.data.TFRecordDataset\n",
    "  \"\"\"\n",
    "\n",
    "  total_tfrecords = sum([len(tf.io.gfile.glob(f)) for f in tfrecords])\n",
    "  num_readers = min(total_tfrecords, max_readers)\n",
    "  dataset = tf.data.Dataset.list_files(tfrecords, shuffle=shuffle)\n",
    "  if repeat:\n",
    "    dataset = dataset.repeat()\n",
    "  return dataset.apply(\n",
    "      tf.data.experimental.parallel_interleave(\n",
    "          tf.data.TFRecordDataset,\n",
    "          num_readers,\n",
    "          sloppy=sloppy,\n",
    "          buffer_output_elements=parallel_threads,\n",
    "          prefetch_input_elements=parallel_threads))\n",
    "\n",
    "\n",
    "def _parse_tfex_proto(example_proto):\n",
    "  \"\"\"Parses the tfexample proto to a raw mesh_data dictionary.\n",
    "  Args:\n",
    "    example_proto: A tf.Example proto storing the encoded mesh data.\n",
    "  Returns:\n",
    "    A mesh data dictionary with the following fields:\n",
    "      'num_vertices': The `int64` number of vertices in mesh.\n",
    "      'num_triangles': The `int64` number of triangles in mesh.\n",
    "      'vertices': A serialized tensor of vertex positions.\n",
    "      'triangles': A serialized tensor of triangle vertex indices.\n",
    "      'labels': A serialized tensor of per vertex class labels.\n",
    "  \"\"\"\n",
    "  feature_description = {\n",
    "      'num_vertices': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "      'num_triangles': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "      'vertices': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "      'triangles': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "      'labels': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "  }\n",
    "  return tf.io.parse_single_example(\n",
    "      serialized=example_proto, features=feature_description)\n",
    "\n",
    "\n",
    "def _parse_mesh_data(mesh_data, mean_center=True):\n",
    "  \"\"\"Parses a raw mesh_data dictionary read from tf examples.\n",
    "  Args:\n",
    "    mesh_data: A mesh data dictionary with serialized data tensors,\n",
    "      as output from _parse_tfex_proto()\n",
    "    mean_center: If true, centers the mesh vertices to mean(vertices).\n",
    "  Returns:\n",
    "     A mesh data dictionary with following fields:\n",
    "      'num_vertices': The `int32` number of vertices in mesh.\n",
    "      'num_triangles': The `int32` number of triangles in mesh.\n",
    "      'num_edges': The `int32` number of unique directed edges in mesh.\n",
    "      'vertices': A [V, 3] `float32` of vertex positions.\n",
    "      'triangles': A [T, 3] `int32` tensor of triangle vertex indices.\n",
    "      'labels': A [V] `int32` tensor of per vertex class labels.\n",
    "      'edges': A [E, 2] `int32` tensor of unique directed edges in mesh.\n",
    "      'edge_weights': A [E] `float32` tensor of vertex degree based edge\n",
    "        weights.\n",
    "  \"\"\"\n",
    "  labels = tf.io.parse_tensor(mesh_data['labels'], tf.int32)\n",
    "  vertices = tf.io.parse_tensor(mesh_data['vertices'], tf.float32)\n",
    "  triangles = tf.io.parse_tensor(mesh_data['triangles'], tf.int32)\n",
    "  if mean_center:\n",
    "    vertices = vertices - tf.reduce_mean(\n",
    "        input_tensor=vertices, axis=0, keepdims=True)\n",
    "\n",
    "  edges, weights = tf.py_function(\n",
    "      func=lambda t: get_weighted_edges(t.numpy()),\n",
    "      inp=[triangles],\n",
    "      Tout=[tf.int32, tf.float32])\n",
    "\n",
    "  num_edges = tf.shape(input=edges)[0]\n",
    "  num_vertices = tf.cast(mesh_data['num_vertices'], tf.int32)\n",
    "  num_triangles = tf.cast(mesh_data['num_triangles'], tf.int32)\n",
    "  mesh_data = dict(\n",
    "      vertices=vertices,\n",
    "      labels=labels,\n",
    "      triangles=triangles,\n",
    "      edges=edges,\n",
    "      edge_weights=weights,\n",
    "      num_triangles=num_triangles,\n",
    "      num_vertices=num_vertices,\n",
    "      num_edges=num_edges)\n",
    "  return mesh_data\n",
    "\n",
    "\n",
    "def create_dataset_from_tfrecords(tfrecords, params):\n",
    "  \"\"\"Creates a mesh dataset given a list of tf records filenames.\n",
    "  Args:\n",
    "    tfrecords: A list of TFRecords filenames.\n",
    "    params: A dictionary of IO paramaters, see DEFAULT_IO_PARAMS.\n",
    "  Returns:\n",
    "    A tf.data.Dataset, with each element a dictionary of batched mesh data\n",
    "      with following fields:\n",
    "      'vertices': A [B, V, 3] `float32` tensor of vertex positions, possibly\n",
    "        0-padded.\n",
    "      'triangles': A [B, T, 3] `int32` tensor of triangle vertex indices,\n",
    "        possibly 0-padded\n",
    "      'labels': A [B, V] `int32` tensor of per vertex class labels, possibly\n",
    "        0-padded\n",
    "      'edges': A [B, E, 2] `int32` tensor of unique directed edges in mesh,\n",
    "        possibly 0-padded\n",
    "      'edge_weights': A [B, E] `float32` tensor of vertex degree based edge\n",
    "        weights, possibly 0-padded.\n",
    "      'num_edges': A [B] `int32` tensor of number of unique directed edges in\n",
    "        each mesh in the batch.\n",
    "      'num_vertices':  A [B] `int32` tensor of number of vertices in each mesh\n",
    "        in the batch.\n",
    "      'num_triangles': A [B] `int32` tensor of number of triangles in each mesh\n",
    "        in the batch.\n",
    "  \"\"\"\n",
    "\n",
    "  def _set_default_if_none(param, param_dict, default_val):\n",
    "    if param not in param_dict:\n",
    "      return default_val\n",
    "    else:\n",
    "      return default_val if param_dict[param] is None else param_dict[param]\n",
    "\n",
    "  is_training = params['is_training']\n",
    "  shuffle = _set_default_if_none('shuffle', params, is_training)\n",
    "  repeat = _set_default_if_none('repeat', params, is_training)\n",
    "  sloppy = _set_default_if_none('sloppy', params, is_training)\n",
    "\n",
    "  if not isinstance(tfrecords, list):\n",
    "    tfrecords = [tfrecords]\n",
    "  dataset = _tfrecords_to_dataset(tfrecords, params['parallel_threads'],\n",
    "                                  shuffle, repeat, sloppy)\n",
    "  dataset = dataset.map(_parse_tfex_proto, tf.data.experimental.AUTOTUNE)\n",
    "  dataset = dataset.map(\n",
    "      lambda x: _parse_mesh_data(x, mean_center=params['mean_center']),\n",
    "      tf.data.experimental.AUTOTUNE)\n",
    "  if repeat:\n",
    "    dataset = dataset.repeat()\n",
    "  if shuffle:\n",
    "    dataset = dataset.shuffle(params['shuffle_buffer_size'])\n",
    "  return dataset.padded_batch(\n",
    "      params['batch_size'],\n",
    "      padded_shapes={\n",
    "          'vertices': [None, 3],\n",
    "          'labels': [None],\n",
    "          'triangles': [None, 3],\n",
    "          'edges': [None, 2],\n",
    "          'edge_weights': [None],\n",
    "          'num_edges': [],\n",
    "          'num_vertices': [],\n",
    "          'num_triangles': [],\n",
    "      },\n",
    "      drop_remainder=is_training)\n",
    "\n",
    "\n",
    "def create_input_from_dataset(dataset_fn, files, io_params):\n",
    "  \"\"\"Creates input function given dataset generator and input files.\n",
    "  Args:\n",
    "    dataset_fn: A dataset generator function.\n",
    "    files: A list of TFRecords filenames.\n",
    "    io_params: A dictionary of IO paramaters, see DEFAULT_IO_PARAMS.\n",
    "  Returns:\n",
    "    features: A dictionary of mesh data training features.\n",
    "    labels: A [B] `int32` tensor of per vertex class labels.\n",
    "  \"\"\"\n",
    "  for k in DEFAULT_IO_PARAMS:\n",
    "    io_params[k] = io_params[k] if k in io_params else DEFAULT_IO_PARAMS[k]\n",
    "\n",
    "  dataset = dataset_fn(files, io_params)\n",
    "  mesh_data = tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n",
    "  mesh_data['neighbors'] = adjacency_from_edges(mesh_data['edges'],\n",
    "                                                mesh_data['edge_weights'],\n",
    "                                                mesh_data['num_edges'],\n",
    "                                                mesh_data['num_vertices'])\n",
    "\n",
    "  max_num_verts = tf.reduce_max(input_tensor=mesh_data['num_vertices'])\n",
    "  features = dict(\n",
    "      vertices=tf.reshape(mesh_data['vertices'], [-1, max_num_verts, 3]),\n",
    "      triangles=mesh_data['triangles'],\n",
    "      neighbors=mesh_data['neighbors'],\n",
    "      edges=mesh_data['edges'],\n",
    "      edge_weights=mesh_data['edge_weights'],\n",
    "      num_triangles=mesh_data['num_triangles'],\n",
    "      num_vertices=mesh_data['num_vertices'],\n",
    "      extra=tf.shape(mesh_data['vertices'])\n",
    "  )\n",
    "  labels = mesh_data['labels']\n",
    "  # Copy labels to features dictionary for estimator prediction mode.\n",
    "  if not io_params['is_training']:\n",
    "    features['labels'] = mesh_data['labels']\n",
    "  return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AGaDtH49dlJb"
   },
   "source": [
    "Note this notebook works best in Graph mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Gh-ZSwXnB-5"
   },
   "source": [
    "### Fetch model files and data\n",
    "\n",
    "For convenience, we provide a pre-trained model. Let's now download a pre-trained model checkpoint and the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ZkB3iIcvvzJ"
   },
   "outputs": [],
   "source": [
    "path_to_model_zip = tf.keras.utils.get_file(\n",
    "    'model.zip',\n",
    "    origin='https://storage.googleapis.com/tensorflow-graphics/notebooks/mesh_segmentation/model.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_data_zip = tf.keras.utils.get_file(\n",
    "    'data.zip',\n",
    "    origin='https://storage.googleapis.com/tensorflow-graphics/notebooks/mesh_segmentation/data.zip',\n",
    "    extract=True)\n",
    "\n",
    "local_model_dir = os.path.join(os.path.dirname(path_to_model_zip), 'model')\n",
    "test_data_files = [\n",
    "    os.path.join(\n",
    "        os.path.dirname(path_to_data_zip),\n",
    "        'data/Dancer_test_sequence.tfrecords')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dmh4b6VKcATt"
   },
   "source": [
    "## Load and visualize test data\n",
    "\n",
    "For graph convolutions, we need a *weighted adjacency matrix* denoting the mesh\n",
    "connectivity. Feature-steered graph convolutions expect self-edges in the mesh\n",
    "connectivity for each vertex, i.e. the diagonal of the weighted adjacency matrix\n",
    "should be non-zero. This matrix is defined as:\n",
    "```\n",
    "A[i, j] = w[i,j] if vertex i and vertex j share an edge,\n",
    "A[i, i] = w[i,i] for each vertex i,\n",
    "A[i, j] = 0 otherwise.\n",
    "where, w[i, j] = 1/(degree(vertex i)), and sum(j)(w[i,j]) = 1\n",
    "```\n",
    "Here degree(vertex i) is the number of edges incident on a vertex (including the\n",
    "self-edge). This weighted adjacency matrix is stored as a SparseTensor.\n",
    "\n",
    "We will load the test meshes from the test [tf.data.TFRecordDataset](https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset)\n",
    "downloaded above. Each mesh is stored as a\n",
    "[tf.Example](https://www.tensorflow.org/api_docs/python/tf/train/Example), with\n",
    "the following fields:\n",
    "\n",
    "*   'num_vertices': Number of vertices in each mesh\n",
    "*   'num_triangles': Number of triangles in each mesh.\n",
    "*   'vertices': A [V, 3] float tensor of vertex positions.\n",
    "*   'triangles': A [T, 3] integer tensor of vertex indices for each triangle.\n",
    "*   'labels': A [V] integer tensor with segmentation class label for each\n",
    "    vertex.\n",
    "\n",
    "where 'V' is number of vertices and 'T' is number of triangles in the mesh. As\n",
    "each mesh may have a varying number of vertices and faces (and the corresponding\n",
    "connectivity matrix), we pad the data tensors with '0's in each batch.\n",
    "\n",
    "For details on the dataset pipeline implementation, take a look at\n",
    "mesh_segmentation_dataio.py.\n",
    "\n",
    "Lets try to load a batch from the test TFRecordDataset, and visualize the first\n",
    "mesh with each vertex colored by the part label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LZM02o0pEny6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0812 15:24:27.235121 4550243776 deprecation.py:323] From <ipython-input-3-50ae7c1b2806>:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
      "W0812 15:24:27.343994 4550243776 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow_graphics/geometry/convolution/utils.py:276: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0812 15:24:27.457840 123145487855616 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "W0812 15:24:27.458195 123145484636160 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "W0812 15:24:27.458306 123145485709312 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "W0812 15:24:27.458475 123145487319040 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "W0812 15:24:27.458572 123145486245888 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n"
     ]
    }
   ],
   "source": [
    "test_io_params = {\n",
    "    'is_training': False,\n",
    "    'sloppy': False,\n",
    "    'shuffle': True,\n",
    "}\n",
    "test_tfrecords = test_data_files\n",
    "\n",
    "input_graph = tf.Graph()\n",
    "with input_graph.as_default():\n",
    "  mesh_load_op = create_input_from_dataset(\n",
    "      create_dataset_from_tfrecords, test_tfrecords, test_io_params)\n",
    "  with tf.Session() as sess:\n",
    "    test_mesh_data, test_labels = sess.run(mesh_load_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jgRp-0fplMBD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Current Index: 5\n",
      ">>>> Vertices: 2662 / 2666\n",
      ">>>> Faces: 4000 / 4000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17c13e1b3ae14312a85e2ed1fc6295fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(background='#dddddd', camera=PerspectiveCamera(aspect=1.5, children=(DirectionalLight(position=(-30.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   0. 233. 300. 475. 472. 815. 272.  83.   8.   3.]\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "if 'cur_idx' not in locals() and 'cur_idx' not in globals():\n",
    "  cur_idx = 0\n",
    "    \n",
    "if cur_idx >= test_mesh_data['num_vertices'].shape[0]:\n",
    "  cur_idx = 0\n",
    "\n",
    "print(\">>> Current Index: %d\" % cur_idx)\n",
    "print(\">>>> Vertices: %d / %d\" % (test_mesh_data['num_vertices'][cur_idx], test_mesh_data[\"vertices\"][cur_idx].shape[0]))\n",
    "print(\">>>> Faces: %d / %d\" % (test_mesh_data[\"num_triangles\"][cur_idx], test_mesh_data[\"triangles\"][cur_idx].shape[0]))\n",
    "\n",
    "input_mesh_data = {\n",
    "    'vertices': test_mesh_data['vertices'][cur_idx],\n",
    "    'faces': test_mesh_data['triangles'][cur_idx],\n",
    "    'vertex_colors': mesh_viewer.SEGMENTATION_COLORMAP[test_labels[cur_idx]],\n",
    "}\n",
    "display_mesh(input_mesh_data)\n",
    "\n",
    "mesh = trimesh.Trimesh(vertices=test_mesh_data['vertices'][cur_idx][:test_mesh_data['num_vertices'][cur_idx]],\n",
    "                       faces=test_mesh_data['triangles'][cur_idx][:test_mesh_data['num_triangles'][cur_idx]])\n",
    "neighbor_num = [len(v) for v in mesh.vertex_neighbors]\n",
    "hist, bins, _ = plt.hist(neighbor_num, bins=range(12), alpha=0.7, rwidth=0.75)  # arguments are passed to np.histogram\n",
    "plt.title(\"Histogram of count of neighbors\")\n",
    "plt.show()\n",
    "\n",
    "# hist, bins = np.histogram(neighbor_num, bins=range(12))\n",
    "print(hist)\n",
    "\n",
    "cur_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aqV6vkCkWB7J"
   },
   "source": [
    "## Model Definition\n",
    "\n",
    "Given a mesh with V vertices and D-dimensional per-vertex input features (e.g.\n",
    "vertex position, normal), we would like to create a network capable of\n",
    "classifying each vertex to a part label. Lets first create a mesh encoder that\n",
    "encodes each vertex in the mesh into C-dimensional logits, where C is the number\n",
    "of parts. First we use 1x1 convolutions to change input feature dimensions,\n",
    "followed by a sequence of feature steered graph convolutions and ReLU\n",
    "non-linearities, and finally 1x1 convolutions to logits, which are used for\n",
    "computing softmax cross entropy as described below.\n",
    "\n",
    "Note that this model does not use any form of pooling, which is outside the scope of this notebook.\n",
    "\n",
    "![](https://storage.googleapis.com/tensorflow-graphics/notebooks/mesh_segmentation/mesh_segmentation_model_def.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fQVeuGazM0LK"
   },
   "outputs": [],
   "source": [
    "MODEL_PARAMS = {\n",
    "    'num_filters': 8,\n",
    "    'num_classes': 16,\n",
    "    'encoder_filter_dims': [32, 64, 128],\n",
    "}\n",
    "\n",
    "\n",
    "def mesh_encoder(batch_mesh_data, num_filters, output_dim, conv_layer_dims):\n",
    "  \"\"\"A mesh encoder using feature steered graph convolutions.\n",
    "\n",
    "    The shorthands used below are\n",
    "      `B`: Batch size.\n",
    "      `V`: The maximum number of vertices over all meshes in the batch.\n",
    "      `D`: The number of dimensions of input vertex features, D=3 if vertex\n",
    "        positions are used as features.\n",
    "\n",
    "  Args:\n",
    "    batch_mesh_data: A mesh_data dict with following keys\n",
    "      'vertices': A [B, V, D] `float32` tensor of vertex features, possibly\n",
    "        0-padded.\n",
    "      'neighbors': A [B, V, V] `float32` sparse tensor of edge weights.\n",
    "      'num_vertices': A [B] `int32` tensor of number of vertices per mesh.\n",
    "    num_filters: The number of weight matrices to be used in feature steered\n",
    "      graph conv.\n",
    "    output_dim: A dimension of output per vertex features.\n",
    "    conv_layer_dims: A list of dimensions used in graph convolution layers.\n",
    "\n",
    "  Returns:\n",
    "    vertex_features: A [B, V, output_dim] `float32` tensor of per vertex\n",
    "      features.\n",
    "  \"\"\"\n",
    "  batch_vertices = batch_mesh_data['vertices']\n",
    "\n",
    "  # Linear: N x D --> N x 16.\n",
    "  vertex_features = tf.keras.layers.Conv1D(16, 1, name='lin16')(batch_vertices)\n",
    "\n",
    "  # graph convolution layers\n",
    "  for dim in conv_layer_dims:\n",
    "    with tf.variable_scope('conv_%d' % dim):\n",
    "      vertex_features = graph_conv.feature_steered_convolution_layer(\n",
    "          vertex_features,\n",
    "          batch_mesh_data['neighbors'],\n",
    "          batch_mesh_data['num_vertices'],\n",
    "          num_weight_matrices=num_filters,\n",
    "          num_output_channels=dim)\n",
    "    vertex_features = tf.nn.relu(vertex_features)\n",
    "\n",
    "  # Linear: N x 128 --> N x 256.\n",
    "  vertex_features = tf.keras.layers.Conv1D(\n",
    "      256, 1, name='lin256')(\n",
    "          vertex_features)\n",
    "  vertex_features = tf.nn.relu(vertex_features)\n",
    "\n",
    "  # Linear: N x 256 --> N x output_dim.\n",
    "  vertex_features = tf.keras.layers.Conv1D(\n",
    "      output_dim, 1, name='lin_output')(\n",
    "          vertex_features)\n",
    "\n",
    "  return vertex_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6c2pz4r_79F_"
   },
   "source": [
    "Given a mesh encoder, let's define a model_fn for a custom\n",
    "[tf.Estimator](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator)\n",
    "for vertex classification using softmax cross entropy loss. A tf.Estimator model_fn returns the ops necessary to perform training, evaluation, or predictions given inputs and a number of other parameters. Recall that the\n",
    "vertex tensor may be zero-padded (see Dataset Pipeline above), hence we must mask out the contribution from the padded values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WE-cuv0i78ak"
   },
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "  \"\"\"Returns a mesh segmentation model_fn for use with tf.Estimator.\"\"\"\n",
    "  size = features['extra']\n",
    "  print(size[1])\n",
    "\n",
    "  logits = mesh_encoder(features, params['num_filters'], params['num_classes'],\n",
    "                        params['encoder_filter_dims'])\n",
    "  predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "  outputs = {\n",
    "      'vertices': features['vertices'],\n",
    "      'triangles': features['triangles'],\n",
    "      'num_vertices': features['num_vertices'],\n",
    "      'num_triangles': features['num_triangles'],\n",
    "      'predictions': predictions,\n",
    "#       'extra': size\n",
    "  }\n",
    "  # For predictions, return the outputs.\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    outputs['labels'] = features['labels']\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=outputs)\n",
    "  # Loss\n",
    "  # Weight the losses by masking out padded vertices/labels.\n",
    "  vertex_ragged_sizes = features['num_vertices']\n",
    "  mask = tf.sequence_mask(vertex_ragged_sizes, tf.shape(labels)[-1])\n",
    "  loss_weights = tf.cast(mask, dtype=tf.float32)\n",
    "  loss = tf.losses.sparse_softmax_cross_entropy(\n",
    "      logits=logits, labels=labels, weights=loss_weights)\n",
    "  # For training, build the optimizer.\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.AdamOptimizer(\n",
    "        learning_rate=params['learning_rate'],\n",
    "        beta1=params['beta'],\n",
    "        epsilon=params['adam_epsilon'])\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "      train_op = optimizer.minimize(\n",
    "          loss=loss, global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "  # For eval, return eval metrics.\n",
    "  eval_ops = {\n",
    "      'mean_loss':\n",
    "          tf.metrics.mean(loss),\n",
    "      'accuracy':\n",
    "          tf.metrics.accuracy(\n",
    "              labels=labels, predictions=predictions, weights=loss_weights)\n",
    "  }\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "94FICCro_dLV"
   },
   "source": [
    "## Test model & visualize results\n",
    "\n",
    "Now that we have defined the model, let's load the weights from the trained model downloaded above and use tf.Estimator.predict to predict the part labels for meshes in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Olj5zIkg72FK"
   },
   "outputs": [],
   "source": [
    "test_io_params = {\n",
    "    'is_training': False,\n",
    "    'sloppy': False,\n",
    "    'shuffle': True,\n",
    "    'repeat': False,\n",
    "    'batch_size': 2\n",
    "}\n",
    "test_tfrecords = test_data_files\n",
    "\n",
    "def predict_fn():\n",
    "  return create_input_from_dataset(create_dataset_from_tfrecords,\n",
    "                                          test_tfrecords,\n",
    "                                          test_io_params)\n",
    "\n",
    "\n",
    "estimator = tf.estimator.Estimator(model_fn=model_fn,\n",
    "                                   model_dir=local_model_dir,\n",
    "                                   params=MODEL_PARAMS)\n",
    "test_predictions = estimator.predict(input_fn=predict_fn)\n",
    "\n",
    "cur_idx = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IO1VmbL087xf"
   },
   "source": [
    "Run the following cell repeatedly to cycle through the meshes in the test sequence. The left view shows the input mesh, and the right view shows the predicted part labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xuoVe70D5PAF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice_2:0\", shape=(), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dcd43f7a1c34973be8595a44dde3e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(background='#dddddd', camera=PerspectiveCamera(aspect=1.5, children=(DirectionalLight(position=(-30.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd052f093e24860baa206d39db1b6cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(background='#dddddd', camera=PerspectiveCamera(aspect=1.5, children=(DirectionalLight(position=(-30.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "prediction = next(test_predictions)\n",
    "# print(prediction[\"extra\"])\n",
    "input_mesh_data = {\n",
    "    'vertices': prediction['vertices'],\n",
    "    'faces': prediction['triangles'],\n",
    "}\n",
    "predicted_mesh_data = {\n",
    "    'vertices': prediction['vertices'],\n",
    "    'faces': prediction['triangles'],\n",
    "    'vertex_colors': mesh_viewer.SEGMENTATION_COLORMAP[prediction['predictions']],\n",
    "}\n",
    "\n",
    "display_mesh(input_mesh_data)\n",
    "display_mesh(predicted_mesh_data)\n",
    "\n",
    "print(cur_idx)\n",
    "cur_idx+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Our Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4vYRrD4-POtR"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a15c4774ab45418448e3cf0b53f92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(background='#dddddd', camera=PerspectiveCamera(aspect=1.5, children=(DirectionalLight(position=(-30.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mesh = trimesh.load(\"../meshes/f_c_10412256613_model.obj\")\n",
    "mesh_data = {\n",
    "    'vertices': mesh.vertices,\n",
    "    'faces': mesh.faces\n",
    "}\n",
    "display_mesh(mesh_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3xYrAxxPa5iE"
   },
   "outputs": [],
   "source": [
    "mean_center = True\n",
    "\n",
    "def mesh_to_features(mesh):\n",
    "  labels = tf.zeros(mesh.vertices.shape[0], tf.int32)\n",
    "  vertices = tf.convert_to_tensor(mesh.vertices, dtype=tf.float32)\n",
    "  num_vertices = tf.shape(input=vertices)[0]\n",
    "  if mean_center:\n",
    "    vertices = vertices - tf.reduce_mean(input_tensor=vertices, axis=0, keepdims=True)\n",
    "\n",
    "  triangles = tf.convert_to_tensor(mesh.faces, dtype=tf.int32)\n",
    "  num_triangles = tf.shape(input=triangles)[0]\n",
    "\n",
    "  edges, edge_weights = tf.py_function(\n",
    "    func=lambda t: dataio.get_weighted_edges(t.numpy()),\n",
    "    inp=[triangles],\n",
    "    Tout=[tf.int32, tf.float32]\n",
    "  )\n",
    "  num_edges = tf.shape(input=edges)[0]\n",
    "\n",
    "  vertices=tf.reshape(vertices, [1, -1, 3])\n",
    "  triangles=tf.reshape(triangles, [1, -1, 3])\n",
    "  edges=tf.reshape(edges, [1, -1, 2])\n",
    "  edge_weights=tf.reshape(edge_weights, [1, -1])\n",
    "  labels = tf.reshape(labels, [1, -1])\n",
    "  \n",
    "  num_vertices = tf.reshape(num_vertices, [1])\n",
    "  num_triangles = tf.reshape(num_triangles, [1])\n",
    "  num_edges = tf.reshape(num_edges, [1])\n",
    "\n",
    "#     paddings = tf.constant([[0, 2667 - vertices.shape[0]], [0, 0]])\n",
    "#     vertices = tf.pad(vertices, paddings, \"CONSTANT\")\n",
    "\n",
    "#     paddings = tf.constant([[0, 4000 - triangles.shape[0]], [0, 0]])\n",
    "#     triangles = tf.pad(vertices, paddings, \"CONSTANT\")\n",
    "\n",
    "#     paddings = tf.constant([[0, 15877 - edges.shape[0]], [0, 0]])\n",
    "#     edges = tf.pad(edges, paddings, \"CONSTANT\")\n",
    "\n",
    "#     paddings = tf.constant([[0, 15877 - weights.shape[0]], [0, 0]])\n",
    "#     weights = tf.pad(weights, paddings, \"CONSTANT\")\n",
    "\n",
    "  neighbors = dataio.adjacency_from_edges(\n",
    "    edges,\n",
    "    edge_weights,\n",
    "    num_edges,\n",
    "    num_vertices\n",
    "  )\n",
    "\n",
    "\n",
    "  features = dict(\n",
    "    vertices=vertices,\n",
    "    triangles=triangles,\n",
    "    neighbors=neighbors,\n",
    "    num_triangles=num_triangles,\n",
    "    num_vertices=num_vertices,\n",
    "    labels=labels\n",
    "  )\n",
    "\n",
    "\n",
    "#   return labels, vertices, triangles, edges, edge_weights, neighbors\n",
    "  return features, labels\n",
    "\n",
    "# input_graph = tf.Graph()\n",
    "# with input_graph.as_default():\n",
    "#   with tf.Session() as sess:\n",
    "#     mesh_op = handle(mesh)\n",
    "#     labels, vertices, triangles, edges, weights, neighbors = sess.run(mesh_op)\n",
    "\n",
    "#     print(vertices.shape)\n",
    "#     print(edges.shape)\n",
    "#     print(weights.shape)\n",
    "#     print(neighbors)\n",
    "    \n",
    "# mesh_data = {\n",
    "#     'vertices': vertices,\n",
    "#     'faces': triangles\n",
    "# }\n",
    "# input_viewer = mesh_viewer.Viewer(mesh_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4lWNHEaNuIyp"
   },
   "outputs": [],
   "source": [
    "def my_predict_fn():\n",
    "  return mesh_to_features(mesh)\n",
    "\n",
    "\n",
    "my_estimator = tf.estimator.Estimator(model_fn=model_fn,\n",
    "                                   model_dir=local_model_dir,\n",
    "                                   params=MODEL_PARAMS)\n",
    "my_test_predictions = my_estimator.predict(input_fn=my_predict_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6VUHIBHuudHG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0808 17:41:16.844424 4485752256 estimator.py:1000] Input graph does not use tf.data.Dataset or contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Vertices: 67149 / 67149\n",
      ">>>> Faces: 100000 / 100000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a06fef75c1b442dad274cbf07ad33a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(background='#dddddd', camera=PerspectiveCamera(aspect=1.5, children=(DirectionalLight(position=(-30.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe53f0220724f05a553e9c8cc8e2a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(background='#dddddd', camera=PerspectiveCamera(aspect=1.5, children=(DirectionalLight(position=(-30.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction = next(my_test_predictions)\n",
    "print(\">>>> Vertices: %d / %d\" % (prediction[\"num_vertices\"], prediction[\"vertices\"].shape[0]))\n",
    "print(\">>>> Faces: %d / %d\" % (prediction[\"num_triangles\"], prediction[\"triangles\"].shape[0]))\n",
    "\n",
    "input_mesh_data = {\n",
    "    'vertices': prediction['vertices'],\n",
    "    'faces': prediction['triangles'],\n",
    "}\n",
    "predicted_mesh_data = {\n",
    "    'vertices': prediction['vertices'],\n",
    "    'faces': prediction['triangles'],\n",
    "    'vertex_colors': mesh_viewer.SEGMENTATION_COLORMAP[prediction['predictions']],\n",
    "}\n",
    "\n",
    "display_mesh(input_mesh_data)\n",
    "display_mesh(predicted_mesh_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of mesh_segmentation_demo.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
